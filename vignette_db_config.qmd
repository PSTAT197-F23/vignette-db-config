---
title: "Configuring a Database and Writing Queries in R"
author: "Ayushmaan Gandhi, Sam Guimte, Maya Hetz, Mario Tapia-Pacheco"
date: "2023-12-10"
output: 
    toc: true
    toc_float: true
---

In this lab youâ€™ll see how to create a database, write SQL queries, and use 
the queried data to build models.

Objectives: create a SQL database with multiple csv files; practice writing 
SQL queries; fit knn and extreme gradient boosting models to queried data.

# Databases
A database is a large collection of data that can be accessed and edited by numerous people simultaneously, and this flexibility is convenient for all scales of projects. The main benefits include the efficient storage and retrieval of data, constantly updated data when connected straight to the source, the allowance of multiple concurrent users, and enhanced data privacy.

We will be working with a relational database in this vignette, which is a database comprising one or more tables (or "relations") of interconnected data. The data within each can be joined with others through merging techniques to create one's own table of just the desired data for data analysis, modeling, etc. 

The data used in this vignette is a "Football Database" from Kaggle that contains football-related data (player and team statistics) covering the Top 5 leagues in Europe from 2014 to 2020. It contains 7 separate csv files that will act as our database's relations: appearances, games, leagues, players, shots, teams, teamstats. There are both continuous and categorical variables in each table that can be leveraged for data exploration and model fitting later in the vignette.

# Configuring a Database

### Setup
:::{.callout-important title="Action"}
Open RStudio and set up a new script in your lab directory. Copy and paste the code chunk below and execute once.

You may need to install the `DBI`, `RSQL`, and `RSQLite` packages.
:::

```{r, warning=FALSE, message=FALSE}
library(DBI)  # Contains functions for interacting with database
library(RSQL)  # Generate and process SQL queries in R
library(RSQLite)  # Can create an in-memory SQL database
library(tidyverse)

# Load each file in 
appearances <- read_csv('data/preprocessed/appearances.csv')
games <- read_csv('data/preprocessed/games.csv')
leagues <- read_csv('data/preprocessed/leagues.csv')
players <- read_csv('data/preprocessed/players.csv')
shots <- read_csv('data/preprocessed/shots.csv')
teams <- read_csv('data/preprocessed/teams.csv')
teamstats <- read_csv('data/preprocessed/teamstats.csv')
```

### Creating the Database

We can start by creating the database connection.

:::{.callout-warning}
If the database is not empty, replace "football" with another name for your 
database.
:::

```{r, warning=FALSE}
# Create the database connection
soccer_con <- dbConnect(drv = RSQLite::SQLite(),
                        dbname = "data/databases/football")

# Check that database is empty
dbListTables(soccer_con)
```

### Populating Database with Tables
We can add tables to our new database using the csv files found in the data 
folder.
```{r}
# Load tables into database
dbWriteTable(conn = soccer_con,
             name = "appearances",
             value= appearances)

dbWriteTable(conn = soccer_con,
             name = "games",
             value= games)

dbWriteTable(conn = soccer_con,
             name = "leagues",
             value= leagues)

dbWriteTable(conn = soccer_con,
             name = "players",
             value= players)

dbWriteTable(conn = soccer_con,
             name = "shots",
             value= shots)

dbWriteTable(conn = soccer_con,
             name = "teams",
             value= teams)

dbWriteTable(conn = soccer_con,
             name = "teamstats",
             value= teamstats)


# Check that database has new tables
dbListTables(soccer_con)
```

### Useful functions
```{r}
dbListFields(soccer_con, "teams")  # list columns in teams dataframe
```

```{r}
sql_teams <- tbl(soccer_con, "teams")  # table from SQL connection
class(sql_teams)
```

```{r}
df_teams <- collect(sql_teams)  # data frame from SQL table
class(df_teams)
```

# Writing Queries

We can use `SELECT` to choose certain variables and `FROM` to specify which 
table in the database.
```{r}
dbGetQuery(soccer_con, "SELECT shooterID, gameID
                        FROM shots") %>%
  head()
```

We can use `*` in the `SELECT` statement to select ALL variables in a table.
```{r}
dbGetQuery(soccer_con, "SELECT *
                        FROM shots") %>%
  head()
```
  
We can use functions on variables in the `SELECT` statement
```{r}
dbGetQuery(soccer_con, "SELECT count(gameID)
                        FROM shots") 
```

We can use a `WHERE` statement to get observations that match a specific 
condition.
```{r}
dbGetQuery(soccer_con, "SELECT count(gameID)
                        FROM shots
                        WHERE shotResult = 'OwnGoal'")
```

We can use an `ORDER BY` statement to order observations.
```{r}
dbGetQuery(soccer_con, "SELECT gameID, shotType
                        FROM shots
                        WHERE shotResult = 'OwnGoal'
                        ORDER BY shooterID desc") %>%
  head()
```

We can use a `HAVING` statement, similar to `WHERE` however it needs to be used 
with a `GROUPBY` statement.
```{r}
dbGetQuery(soccer_con, "SELECT gameID
                        FROM shots
                        WHERE shotResult = 'OwnGoal'
                        GROUP BY gameID
                        HAVING shotType = 'LeftFoot'") %>%
  head()
```

### Joining Tables

There are several kinds of SQL joins that allow us to join tables and extract
information that is stored separately.
```{r echo=FALSE, out.width = "60%", fig.align = "center"}
knitr::include_graphics("images/sql_joins.png")
```

In order to join tables, we need what is called primary keys. This key will allow 
us to relate two or more tables and join them. A primary key can uniquely identify
an observation and a foreign key establishes a relationship with another table's
primary key.

We can use a left join: 
```{r}
dbGetQuery(soccer_con, "SELECT count(homeGoals)
                        FROM games
                        LEFT JOIN shots
                        ON games.gameID = shots.gameID
                        WHERE shotResult = 'OwnGoal'") %>%
  head()
```

An inner join:
```{r}
dbGetQuery(soccer_con, "SELECT count(homeGoals)
                        FROM games
                        INNER JOIN shots
                        ON games.gameID = shots.gameID
                        WHERE shotResult = 'OwnGoal'") %>%
  head()
```

Inspect the data and try joining multiple tables, here's an example.
```{r}
dbGetQuery(soccer_con, "SELECT DISTINCT name 
                        FROM players
                        JOIN shots
                        ON players.playerID = shots.shooterID
                        JOIN games
                        ON shots.gameID = games.gameID
                        WHERE season = '2015'") %>%
  head()
# players who took shots in 2015
```

Let's go through some more example queries and introduce a few more essential
concepts.

### Concept 1: Creating Columns
Creating and modifying columns enables us to derive new data from existing 
datasets, enhancing our ability to analyze and understand our data.

#### Example 1.1
This query adds together the yellowCard and redCard columns to 
create a new column totalCards. Thus, it indicates the total number of cards a 
player received in a game.
```{r}
dbGetQuery(soccer_con, "SELECT playerID, gameID, (yellowCard + redCard) AS
                        totalCards
                        FROM appearances
                        LIMIT 15")
```

#### Example 1.2
This query adds the homeGoals and awayGoals columns to create a 
new column totalGoals. Thus, it indicates the total number of goals scored in each
game.
```{r}
dbGetQuery(soccer_con, "SELECT gameID, homeGoals, awayGoals, homeGoals + awayGoals                         AS totalGoals
                        FROM games
                        LIMIT 15")
```

#### Example 1.3
This query creates a new column uniqueID by concatenating 
playerID and gameID with a hyphen in between.
```{r}
dbGetQuery(soccer_con, "SELECT playerID, gameID, playerID || '-' || gameID AS                             uniqueID
                        FROM appearances
                        LIMIT 15")
```

### Concept 2: Aggregation Functions
Aggregation functions enable us to find averages, maximums, minimums, counts, sums, etc. 

#### Example 2.1
This query calculates the AVERAGE number of goals scored by each team in each 
season.
```{r}
dbGetQuery(soccer_con, "SELECT teamID, season, AVG(goals) AS average_goals
                        FROM teamstats
                        GROUP BY teamID, season
                        LIMIT 15")
```

#### Example 2.2
This query finds the MAXIMUM and MINIMUM number of fouls committed in a single 
game across all records.
```{r}
dbGetQuery(soccer_con, "SELECT MAX(fouls) AS max_fouls, MIN(fouls) AS min_fouls
                        FROM teamstats")
```

#### Example 2.3
This query COUNTS the number of wins ('W'), losses ('L'), and draws ('D') for each
team.
```{r}
dbGetQuery(soccer_con, "SELECT teamID, result, COUNT(*) AS game_count
                        FROM teamstats
                        GROUP BY teamID, result
                        LIMIT 15")
```

#### Example 2.4
This query SUMS up the total number of shots and shots on target for home ('h') 
and away ('a') games separately.
```{r}
dbGetQuery(soccer_con, "SELECT location, SUM(shots) AS total_shots, SUM(shotsOnTarget) AS shots_on_target
                        FROM teamstats
                        GROUP BY location")
```

# Concept 3: Subqueries
Subqueries are a powerful feature that allow us to use the result of one query as 
a condition or reference in another. Thus, it enables complex and layered data 
analysis within a single statement.

#### Example 3.1
This query finds the percentage of each type of shotResult within the Shots table.
In this example, the subquery is used to calculate the total number of shots in 
the dataset. This is a necessary step to determine the percentage of each shot 
result.
```{r}
dbGetQuery(soccer_con, "SELECT shotResult, 
                               COUNT(*) * 100.0 / (SELECT COUNT(*) FROM shots) AS percentage
                        FROM shots
                        GROUP BY shotResult")
```

#### Example 3.2
In this query, the subquery shooterAverages calculates the average xGoal for each 
shooterID. The main query then selects those shooters whose average xGoal is 
higher than the overall average xGoal calculated across all shots. Ultimately, we 
find the shooters that have a higher average xGoal than the average xGoal across 
the table.
```{r}
dbGetQuery(soccer_con, "SELECT shooterID AS bestShooters
                        FROM (
                              SELECT shooterID, AVG(xGoal) AS avgShooterXGoal
                              FROM shots
                              GROUP BY shooterID) AS shooterAverages
                        WHERE avgShooterXGoal > (
                              SELECT AVG(xGoal) FROM shots)
                        LIMIT 15")
```

# Predictive Modeling Using Queried Data
:::{.callout-important title="Action"}
Copy and paste the code chunk below and execute once.

You may need to install some packages.
:::
```{r, warning=FALSE, message=FALSE}
library(tidymodels)
library(themis)
library(ggplot2)
library(corrplot)
library(kknn)
library(ranger)
library(xgboost)
library(vip)
```

We can practice modeling and using queries. Here, we'll build two models that 
aim to predict whether or not a team will win a game.

Let's start by creating a query to get a data frame with relevant data.
```{r}
mdl_data <- dbGetQuery(soccer_con, "SELECT *
                        FROM games
                        FULL OUTER JOIN teamstats
                        ON games.gameID = teamstats.gameID") %>%
  collect()
```

### Explore and Process Data for Modeling
```{r}
# collect relevant columns
mdl_data <- mdl_data %>%
  select(c(36,39:50))

# factor outcome variable
mdl_data$result <- factor(mdl_data$result, levels = c("L", "W", "D"), 
                          labels = c(0,1,2))

# analyze correlation between variables
mdl_data %>% 
  select(where(is.numeric)) %>% 
  cor() %>% 
  corrplot(type = 'lower', diag = FALSE, 
           method = 'color')

# check for class imbalances
mdl_data %>%
  ggplot() +
  geom_bar(aes(x = result, fill = result), color = "black") +
  theme_minimal()
```

Split the data into training and testing sets. Also, create a 5-fold cross 
validation object.
```{r}
# set seed for reproducibility
set.seed(1208)

# split data into training, testing, and create 5 folds for cv
mdl_split <- initial_split(mdl_data, strata = "result", prop = 0.8)

mdl_train <- training(mdl_split)
mdl_test <- testing(mdl_split)

mdl_folds <- vfold_cv(mdl_train, strata = "result", v = 5)
```

Create the recipe for the models.
```{r}
# create recipe
mdl_recipe <- recipe(
  result ~ goals + shots + shotsOnTarget + deep + corners, 
  data = mdl_train) %>% 
  step_upsample(result) %>%
  step_dummy(all_nominal_predictors()) %>%  # dummy-code all nominal preds
  step_normalize(all_numeric_predictors())  # normalize preds

prep(mdl_recipe) %>% bake(new_data = mdl_train)  # prep and bake recipe
```

### K-Nearest Neighbors
K-nearest neighbors is a model that uses information about the observations
closest to our new observations to classify them. It can be used for both 
regression and classification problems, but we will use it for classification. 
The model gets information about the "k" nearest data points and classifies 
new data points based on the majority.
```{r echo=FALSE, out.width = "60%", fig.align = "center"}
knitr::include_graphics("images/knn_model.png")
```

Build a KNN model.
```{r}
# create knn model
knn_model <- nearest_neighbor(neighbors=tune()) %>% # tune n
  set_engine("kknn") %>% 
  set_mode("classification")

# create knn workflow
knn_wflow <- workflow() %>%
  add_model(knn_model) %>%
  add_recipe(mdl_recipe)

# create grid to tune neighbors
knn_tune_grid <- grid_regular(neighbors(range = c(10,500)),
                              levels = 5)

# tune neighbors
# tune_knn <- tune_grid(
#   knn_wflow,
#   resamples = mdl_folds,
#   grid = knn_tune_grid
# )

# write results to rds file to reduce knit time
# write_rds(tune_knn, file = 'data/tuning/knn_tune.rds')

# read outputted rds file
tune_knn <- read_rds('data/tuning/knn_tune.rds')
```

Inspect the results.
```{r}
# visualize model performance by number of neighbors
autoplot(tune_knn)

# manually inspect model performance by number of neighbors
collect_metrics(tune_knn)
```

Choose best performing KNN model and finalize model.
```{r}
# select best performing model based on accuracy
best_knn <- select_best(tune_knn,
                        metric = "accuracy",
                        neighbors
)

# finalize workflow
final_knn_wf <- finalize_workflow(knn_wflow,
                                  best_knn)

# fit model to training data
final_knn <- fit(final_knn_wf, 
                 data = mdl_train)
```

Evaluate performance on training data.
```{r}
# evaluate performance
knn_auc <- augment(final_knn, new_data = mdl_train) %>%
  roc_auc(result, .pred_0:.pred_2)
knn_acc <- augment(final_knn, new_data = mdl_train) %>%
  accuracy(result, .pred_class)
knn_train_results <- bind_rows(knn_auc, knn_acc) %>%
  tibble() %>% mutate(metric = c("roc auc", "acc")) %>%
  select(metric, .estimate)

# display
knn_train_results
```

### Extreme Gradient Boosting

Extreme gradient boosting is an algorithm that uses multiple decision
trees. It takes in training data, uses it to train a model, and then 
evaluates the model on new data. This process repeats until the model 
stops improving.
```{r echo=FALSE, out.width = "60%", fig.align = "center"}
knitr::include_graphics("images/xg_boost.png")
```

Build an extreme gradient boosting model.
```{r}
# create xg boost model
xg_model <- boost_tree(mtry = tune(), 
                       trees = tune(), 
                       learn_rate = tune()) %>%
  set_engine("xgboost") %>% 
  set_mode("classification")

# create xg boost workflow
xg_wflow <- workflow() %>% 
  add_model(xg_model) %>% 
  add_recipe(mdl_recipe)

# create grid to tune hyper parameters
xg_tune_grid <- grid_regular(mtry(range = c(1, 6)), 
                             trees(range = c(200, 600)),
                             learn_rate(range = c(-10, -1)),
                             levels = 5)

# tune hyper parameters
# tune_xg <- tune_grid(
#   xg_wflow,
#   resamples = mdl_folds,
#   grid = xg_tune_grid
# )

# write results to rds file to reduce knit time
# write_rds(tune_xg, file = 'data/tuning/xg_tune.rds')

# read outputted rds file
tune_xg <- read_rds('data/tuning/xg_tune.rds')
```

Inspect the results.
```{r}
# visualize model performance by number of neighbors
autoplot(tune_xg)

# manually inspect model performance by number of neighbors
collect_metrics(tune_xg)
```

Select the best performing extreme gradient boosting model.
```{r}
# select best performing model based on accuracy
best_xg <- select_best(tune_xg,
                       metric = "roc_auc",
                       mtry,
                       trees,
                       learn_rate
)

# finalize workflow
final_xg_wf <- finalize_workflow(xg_wflow,
                                 best_xg)

# fit model to training data
final_xg <- fit(final_xg_wf, 
                data = mdl_train)
```

Evaluate performance on training data.
```{r}
# evaluate performance
xg_auc <- augment(final_xg, new_data = mdl_train) %>%
  roc_auc(result, .pred_0:.pred_2)
xg_acc <- augment(final_xg, new_data = mdl_train) %>%
  accuracy(result, .pred_class)
xg_train_results <- bind_rows(xg_auc, xg_acc) %>%
  tibble() %>% mutate(metric = c("roc auc", "acc")) %>%
  select(metric, .estimate)

# display
xg_train_results
```

### Choosing Best Model Based on Training Data

We can now compare the two models' performances and select the one that 
performs better.
```{r}
# add model names to results
knn_train_results <- knn_train_results %>%
  mutate(model = "K-Nearest Neighbors")
xg_train_results <- xg_train_results %>%
  mutate(model = "Extreme Gradient Boosting")

# combine into one table to easily compare
combined_train_results <- rbind(knn_train_results,
                                xg_train_results)

# display
combined_train_results
```

The KNN model performed better on the training data. We can now check its 
performance on the testing data set.
```{r}
# evaluate performance
knn_test_auc <- augment(final_knn, new_data = mdl_test) %>%
  roc_auc(result, .pred_0:.pred_2)
knn_test_acc <- augment(final_knn, new_data = mdl_test) %>%
  accuracy(result, .pred_class)
knn_test_results <- bind_rows(knn_test_auc, knn_test_acc) %>%
  tibble() %>% mutate(metric = c("roc auc", "acc")) %>%
  select(metric, .estimate)

# display
knn_test_results
```

```{r}
# visualize ROC curve
final_knn_test <- augment(final_knn, mdl_test) %>% 
  select(result, starts_with(".pred")) 

final_knn_test %>% 
  roc_curve(result, .pred_0:.pred_2) %>% 
  autoplot()
```

```{r}
# visualize confusion matrix
conf_mat(final_knn_test, truth = result, 
         .pred_class) %>% 
  autoplot(type = "heatmap")
```

How did the model perform? What kinds of predictors do you think would be 
useful to incorporate into the models? Can you create those predictors with 
SQL queries?

Make sure to close the database connection when you're done to conserve memory.
```{r}
DBI::dbDisconnect(soccer_con)
```
